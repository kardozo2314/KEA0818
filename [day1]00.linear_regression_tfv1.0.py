{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPsf/BDcMu+rBpg5i2e4pfD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 1. Tensorflow 라이브러리 설치\n","##### TensorFlow는 구글이 개발한 머신러닝 및 딥러닝 오픈소스 프레임워크입니다. 🧠 쉽게 말해, 인공지능 모델을 만들고 훈련시키는 데 필요한 다양한 도구를 모아놓은 강력한 라이브러리라고 할 수 있죠. 복잡한 수학 연산과 대규모 데이터 처리를 효율적으로 수행할 수 있도록 설계되었습니다.\n","\n","### 1.1 주요 특징 및 장점 👍\n"," - 유연성과 확장성: CPU, GPU는 물론, 모바일 기기나 엣지 디바이스에서 모델을 실행할 수 있도록 지원하는 TensorFlow Lite, 웹 브라우저에서 실행 가능한 TensorFlow.js 등 다양한 플랫폼을 지원합니다. 연구를 위한 복잡한 모델부터 실제 서비스 배포까지 모두 가능합니다.\n","\n"," - 강력한 생태계: 모델의 학습 과정을 시각화하고 분석하는 TensorBoard, 고수준 API로 모델을 쉽게 만들 수 있는 Keras, 사전 훈련된 모델을 공유하는 TensorFlow Hub 등 강력하고 다양한 도구들을 제공하여 개발 생산성을 크게 높여줍니다.\n","\n"," - 풍부한 자료와 커뮤니티: 구글의 지원을 받는 만큼 공식 문서, 튜토리얼, 예제 코드가 매우 풍부합니다. 또한, 전 세계의 수많은 개발자가 사용하는 덕분에 문제가 발생했을 때 해결책을 찾기 쉬운 거대한 커뮤니티를 가지고 있습니다.\n","\n","### 1.2 주요 사용 분야 🚀\n"," TensorFlow는 거의 모든 머신러닝 및 딥러닝 분야에서 활용될 수 있습니다.\n","\n"," - 이미지 인식 및 분류: 사진 속 객체(고양이, 자동차 등)를 식별하거나, 의료 영상(CT, MRI)을 분석하여 질병을 진단합니다.\n","\n"," - 자연어 처리 (NLP): 문장의 의미를 파악하여 번역하거나(Google 번역), 감정을 분석하고, 챗봇을 만드는 데 사용됩니다.\n","\n"," - 음성 인식: 사용자의 음성을 텍스트로 변환하는 기술(Google 어시스턴트 등)에 활용됩니다.\n","\n"," - 시계열 예측: 주식 가격, 날씨, 전력 수요 등 시간의 흐름에 따른 데이터를 분석하고 미래를 예측합니다.\n","\n"," - 추천 시스템: 사용자의 과거 행동 데이터를 기반으로 좋아할 만한 영화나 상품을 추천합니다.\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"rGGWki7IarO3"}},{"cell_type":"code","source":["# 'pip'이라는 파이썬 패키지 관리자를 사용하여 tensorflow' 라이브러리를 설치합니다.\n","# '!'는 코랩(Colab)이나 주피터 노트북(Jupyter Notebook)과 같은 환경에서 터미널(명령 프롬프트) 명령어를 직접 실행할 때 사용하는 특수 문자입니다.\n","# 즉, 파이썬 코드가 아니라 시스템에 명령을 내리는 부분입니다.\n","# 따라서, 이 코드를 실행하면 TensorFlow 라이브러리가 현재 환경에 다운로드 및 설치됩니다.\n","# TensorFlow가 이미 설치되어 있다면, 기존 버전 정보를 보여주거나 최신 버전으로 업데이트할 수 있습니다.\n","\n","!pip install tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OAMl4sblctEP","executionInfo":{"status":"ok","timestamp":1755319428033,"user_tz":-540,"elapsed":7839,"user":{"displayName":"똥깡아제","userId":"13219049976116428546"}},"outputId":"abefab09-19e6-44cc-f64d-12f5fba66083"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.19.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.3)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n","Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.10.0)\n","Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n","Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.3)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"]}]},{"cell_type":"markdown","source":["# 2. 선형 회귀(Linear Regression) 모델 테스트\n","##### 이 코드는 주어진 데이터(x_train, y_train)를 보고 y = 2x 라는 규칙을 스스로 학습하는 가장 기본적인 선형 회귀(Linear Regression) 모델의 예시입니다.\n","\n","\n","\n","\n"],"metadata":{"id":"iZeNSr27lJhI"}},{"cell_type":"markdown","source":["소스 코드 분석: 머신러닝의 3단계 패턴\n","## 2.1 가설(Hypothesis) 정의: 모델의 수학적 표현\n","이론: 머신러닝의 첫 단계는 우리가 풀고자 하는 문제의 관계를 수학적인 식으로 표현하는 것입니다. 즉, 데이터의 패턴을 가장 잘 나타낼 것이라고 가정하는 모델을 만드는 단계입니다. 이 코드에서는 입력 x와 출력 y 사이에 y = Wx + b 라는 1차 함수(직선) 관계가 있을 것이라고 가설을 세웠습니다. 여기서 머신러닝이 학습해야 할 파라미터 θ는 바로 W(가중치, Weight)와 b(편향, Bias)입니다."],"metadata":{"id":"x4kuaJaulw7o"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# 선형회귀 모델(Wx + b)을 위한 tf.Variable을 선언합니다.\n","# W와 b는 모델이 학습을 통해 스스로 찾아내야 하는 값(파라미터)입니다.\n","# 따라서 계속 값이 변해야 하므로 tf.Variable로 선언합니다.\n","# tf.random.normal(shape=[1])은 W와 b를 임의의 작은 수(정규분포)로 초기화하는 부분입니다.\n","W = tf.Variable(tf.random.normal(shape=[1]))\n","b = tf.Variable(tf.random.normal(shape=[1]))\n","\n","# 이 함수가 바로 우리의 가설, y' = Wx + b 를 코드로 구현한 부분입니다.\n","# 입력 x를 받아 예측값 y'(y_pred)를 반환합니다.\n","@tf.function\n","def linear_model(x):\n","  return W*x + b"],"metadata":{"id":"Nz8L4xugZ6mA","executionInfo":{"status":"ok","timestamp":1755319441449,"user_tz":-540,"elapsed":8563,"user":{"displayName":"똥깡아제","userId":"13219049976116428546"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["- W와 b를 tf.Variable로 선언하는 것은 이 값들이 학습의 대상임을 명시하는 것입니다. 처음에는 무작위 값으로 시작하지만, 훈련을 거치며 최적의 값(정답에 가까운 W=2, b=0)으로 점차 업데이트됩니다.(아래 정답 기준으로 설명)\n","\n","- linear_model(x) 함수는 우리의 가설 h(θ)를 코드로 옮긴 모델 그 자체입니다.\n","\n","\n"],"metadata":{"id":"2Z6gnZyJnjUv"}},{"cell_type":"markdown","source":["## 2.2 손실 함수(Loss Function) 정의: 모델의 오차 측정\n","이론: 가설을 세웠다면, 그 가설이 얼마나 정답에 가까운지, 즉 얼마나 '틀렸는지'를 측정할 기준이 필요합니다. 이 기준을 손실 함수(또는 비용 함수, Cost Function) 라고 합니다. 손실 함수의 값이 클수록 모델의 예측이 많이 틀렸다는 의미이며, 작을수록 정답에 가깝다는 뜻입니다. 머신러닝의 목표는 이 손실 함수의 값을 최소로 만드는 것입니다."],"metadata":{"id":"owOZZHLQn8Su"}},{"cell_type":"code","source":["# 손실 함수를 정의합니다.\n","# MSE 손실함수 \\mean{(y' - y)^2}\n","@tf.function\n","def mse_loss(y_pred, y):\n","  # tf.square(y_pred - y) : 예측값(y_pred)과 실제값(y)의 차이를 제곱하여 오차를 계산합니다. (오차의 부호를 없애고, 오차가 클수록 패널티를 더 많이 주기 위함)\n","  # tf.reduce_mean(...) : 모든 데이터에 대한 오차 제곱의 평균을 계산합니다. 이것이 바로 MSE(Mean Squared Error, 평균 제곱 오차)입니다.\n","  return tf.reduce_mean(tf.square(y_pred - y))"],"metadata":{"id":"hmdLWyHQoTKF","executionInfo":{"status":"ok","timestamp":1755319449672,"user_tz":-540,"elapsed":46,"user":{"displayName":"똥깡아제","userId":"13219049976116428546"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["\n","- mse_loss 함수는 손실 함수 J(θ)를 코드로 구현한 부분입니다.\n","\n","- 모델의 예측값(y_pred)과 실제 정답(y)을 받아, 그 차이(오차)의 평균을 숫자로 반환합니다. 이 숫자가 바로 '손실(loss)'이며, 우리 모델의 현재 성적표입니다.\n","\n","\n"],"metadata":{"id":"FvVKeMJ4oojM"}},{"cell_type":"markdown","source":["## 2.3 학습 알고리즘 설계: 손실(오차) 최소화 과정\n","이론: 이제 모델(가설)이 있고, 그 모델의 성적을 매기는 기준(손실 함수)도 생겼습니다. 마지막 단계는 손실을 줄이기 위해 모델의 파라미터(W, b)를 어떻게 업데이트할지에 대한 방법을 설계하는 것입니다. 가장 대표적인 방법이 바로 경사 하강법(Gradient Descent) 입니다. 손실 함수를 파라미터(W, b)에 대해 미분하여 현재 위치에서 손실이 가장 가파르게 줄어드는 방향(기울기, Gradient)을 계산하고, 그 방향으로 파라미터를 조금씩 이동시키는 작업을 반복합니다."],"metadata":{"id":"_NOXpfvWpBsE"}},{"cell_type":"code","source":["# 최적화를 위한 그라디언트 디센트 옵티마이저를 정의합니다.\n","# SGD는 경사 하강법의 한 종류이며, 0.01은 학습률(learning rate)입니다.\n","# 학습률은 파라미터를 업데이트할 때 얼마나 큰 보폭으로 움직일지를 결정합니다.\n","optimizer = tf.optimizers.SGD(0.01)\n","\n","# 최적화를 위한 function을 정의합니다.\n","@tf.function\n","def train_step(x, y):\n","  # tf.GradientTape는 '기록 테이프'처럼, 이 블록 안에서 일어나는 모든 연산을 기록합니다.\n","  # 이 기록을 이용해 나중에 미분값(기울기)을 자동으로 계산할 수 있습니다.\n","  with tf.GradientTape() as tape:\n","    # 1. 예측 (Hypothesis)\n","    y_pred = linear_model(x)\n","    # 2. 손실 계산 (Loss)\n","    loss = mse_loss(y_pred, y)\n","\n","  # 3. 기울기 계산 (Minimize)\n","  # tape.gradient(손실, [업데이트할 파라미터들])\n","  # 기록된 연산을 바탕으로, 'loss'를 줄이기 위해 'W'와 'b'를 어느 방향으로 얼마나 움직여야 하는지(기울기) 계산합니다.\n","  gradients = tape.gradient(loss, [W, b])\n","\n","  # 4. 파라미터 업데이트 (Minimize)\n","  # 계산된 기울기(gradients)에 따라 optimizer가 W와 b의 값을 업데이트(최적화)합니다.\n","  optimizer.apply_gradients(zip(gradients, [W, b]))\n"],"metadata":{"id":"tMJzy4mhdRkq","executionInfo":{"status":"ok","timestamp":1755319453022,"user_tz":-540,"elapsed":16,"user":{"displayName":"똥깡아제","userId":"13219049976116428546"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["- optimizer: W와 b를 업데이트할 방법론(경사 하강법) 과 학습률(보폭) 을 정의합니다.\n","\n","- train_step 함수: 실제 학습이 일어나는 핵심 과정입니다.\n","\n","1. GradientTape로 연산을 기록합니다.\n","\n","2. 현재 W, b로 예측값(y_pred)을 만들고, 정답(y_train)과 비교하여 손실(loss)을 계산합니다.\n","\n","3. tape.gradient()를 통해 이 손실을 줄이기 위한 W와 b의 기울기(gradient)를 구합니다.\n","\n","4. optimizer.apply_gradients()를 통해 계산된 기울기 방향으로 W와 b의 값을 아주 조금 수정합니다."],"metadata":{"id":"dIvjN0Ppp4a0"}},{"cell_type":"markdown","source":["최종 훈련 및 테스트\n"],"metadata":{"id":"eUtfuHZcqQAb"}},{"cell_type":"code","source":["# 트레이닝을 위한 입력값과 출력값을 준비합니다.\n","x_train = [1, 2, 3, 4]\n","y_train = [2, 4, 6, 8] # 정답 데이터 (y = 2x)\n","\n","# 경사하강법을 1000번 수행합니다.\n","for i in range(1000):\n","  # train_step 함수를 반복 호출하면서 W와 b를 점진적으로 최적의 값으로 업데이트합니다.\n","  train_step(x_train, y_train)\n","\n","# 테스트를 위한 입력값을 준비합니다.\n","x_test = [3.5, 5, 5.5, 6]\n","\n","# 테스트 데이터를 이용해 학습된 선형회귀 모델이 데이터의 경향성(y=2x)을 잘 학습했는지 측정합니다.\n","# 예상되는 참값 : [7, 10, 11, 12]\n","print(linear_model(x_test).numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pCQkjzt7qGOb","executionInfo":{"status":"ok","timestamp":1755319511350,"user_tz":-540,"elapsed":1500,"user":{"displayName":"똥깡아제","userId":"13219049976116428546"}},"outputId":"b5da30a7-268d-4bff-a157-2f48da6bf43b"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 6.9999976  9.999994  10.999993  11.999992 ]\n"]}]},{"cell_type":"markdown","source":["for 루프는 train_step 함수를 1000번 반복 호출합니다. 이 과정을 통해 처음에는\n","무작위 값이던 W와 b가 점차 정답인 W=2, b=0에 매우 가까워집니다.\n","마지막에 새로운 데이터 x_test를 넣어 얻은 결과가 실제 정답과 거의 일치하는 것을 통해, 모델이 데이터의 패턴을 성공적으로 학습했음을 확인할 수 있습니다."],"metadata":{"id":"FAJCaHlqCbp5"}}]}