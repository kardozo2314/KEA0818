{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOF3LrmUfWrEF9edpX6wClK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# RNN - CHAR-RNN 실습\n","##### Char-RNN을 직접 구현하는 종합 실습 코드입니다. 이 코드는 셰익스피어의 글을 학습해서, 셰익스피어 스타일의 새로운 글을 창작해내는 인공지능 작가를 만드는 전체 과정을 담고 있습니다.\n","\n","**실습: 셰익스피어가 되어 글을 쓰는 인공지능 만들기**\n","- 실습에서는 셰익스피어의 모든 작품을 우리 모델에게 읽게 한 뒤, 'ROMEO:' 라고 첫 단어를 던져주면 그 뒤를 이어 새로운 비극을 써 내려가는, 작은 셰익스피어를 탄생시켜 보겠습니다!\n","\n"],"metadata":{"id":"rGGWki7IarO3"}},{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OAMl4sblctEP","executionInfo":{"status":"ok","timestamp":1755411745924,"user_tz":-540,"elapsed":4067,"user":{"displayName":"똥깡아제","userId":"13219049976116428546"}},"outputId":"8a6e7d85-9bd1-459e-8682-2536cfbe1147","collapsed":true},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.19.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.3)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n","Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.10.0)\n","Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n","Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.3)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"]}]},{"cell_type":"markdown","source":["# 1. 패키지 호출"],"metadata":{"id":"ylasWT-eSLgG"}},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","# Python 2와 Python 3 간의 호환성을 높여주는 특별한 import 문입니다.\n","# 코드의 첫 부분에 위치해야 합니다.\n","from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","# 딥러닝 및 머신러닝 모델을 구축하고 학습하는 데 사용되는 핵심 라이브러리인 TensorFlow를 가져옵니다.\n","# 일반적으로 'tf'라는 별칭(alias)으로 사용합니다.\n","import tensorflow as tf\n","\n","# 과학 계산 및 다차원 배열 처리를 위한 필수 라이브러리인 NumPy를 가져옵니다.\n","# 'np'라는 별칭으로 사용하는 것이 관례입니다.\n","import numpy as np\n","\n","# 운영체제와 상호작용할 수 있는 기능을 제공하는 모듈입니다.\n","# 예를 들어, 파일 경로를 다루거나 디렉토리를 생성하는 등의 작업을 할 때 사용됩니다.\n","import os\n","\n","# 시간과 관련된 기능을 제공하는 모듈입니다.\n","# 코드 실행 시간을 측정하거나, 프로그램의 실행을 잠시 멈추는 등의 용도로 사용됩니다.\n","import time"],"metadata":{"id":"32CP03ASRu_D","executionInfo":{"status":"ok","timestamp":1755411748596,"user_tz":-540,"elapsed":17,"user":{"displayName":"똥깡아제","userId":"13219049976116428546"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## 2. Util 정의 및 초기 설정값 지정"],"metadata":{"id":"x4kuaJaulw7o"}},{"cell_type":"code","source":["# input 데이터와 input 데이터를 한글자씩 뒤로 민 target 데이터를 생성하는 utility 함수를 정의합니다.\n","def split_input_target(chunk):\n","  input_text = chunk[:-1]\n","  target_text = chunk[1:]\n","\n","  return input_text, target_text"],"metadata":{"id":"Nz8L4xugZ6mA","executionInfo":{"status":"ok","timestamp":1755411751067,"user_tz":-540,"elapsed":10,"user":{"displayName":"똥깡아제","userId":"13219049976116428546"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# 학습에 필요한 설정값들을 지정합니다.\n","\n","#세익스피어 희곡 데이터 다운로드 학습에 필요한 설정값들을 지정합니다.\n","\n","#세익스피어 희곡 데이터 다운로드\n","data_dir = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')  # shakespeare\n","\n","# 모델 학습 시 한 번에 처리할 데이터 묶음(배치)의 크기를 64로 설정합니다.\n","batch_size = 64\n","\n","# 모델이 다음 글자를 예측하기 위해 참고할 입력 문장의 길이를 100으로 지정합니다.\n","seq_length = 100\n","\n","# 각 글자를 컴퓨터가 이해할 수 있는 256차원의 숫자 벡터로 변환(임베딩)합니다.\n","embedding_dim = 256\n","\n","# 신경망 모델의 은닉층(hidden layer) 크기를 1024개의 노드로 설정하여 모델의 복잡도를 결정합니다.\n","hidden_size = 1024\n","\n","# 전체 데이터셋을 총 10번 반복하여 모델을 학습시킬 횟수(에포크)를 정의합니다.\n","num_epochs = 10"],"metadata":{"id":"dRCtfvpOS7fK","executionInfo":{"status":"ok","timestamp":1755411752252,"user_tz":-540,"elapsed":5,"user":{"displayName":"똥깡아제","userId":"13219049976116428546"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["## 3. 학습 설정값 정의"],"metadata":{"id":"owOZZHLQn8Su"}},{"cell_type":"code","source":["# 학습에 사용할 txt 파일을 읽습니다.\n","text = open(data_dir, 'rb').read().decode(encoding='utf-8')\n","\n","# 학습데이터에 포함된 모든 character들을 나타내는 변수인 vocab과 vocab에 id를 부여해 dict 형태로 만든 char2idx를 선언합니다.\n","vocab = sorted(set(text))  # 유니크한 character 개수\n","vocab_size = len(vocab)\n","print('{} unique characters'.format(vocab_size))\n","char2idx = {u: i for i, u in enumerate(vocab)}\n","idx2char = np.array(vocab)\n","\n","# 학습 데이터를 character에서 integer로 변환합니다.\n","text_as_int = np.array([char2idx[c] for c in text])\n","\n","# split_input_target 함수를 이용해서 input 데이터와 input 데이터를 한글자씩 뒤로 민 target 데이터를 생성합니다.\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n","dataset = sequences.map(split_input_target)\n","\n","# tf.data API를 이용해서 데이터를 섞고 batch 형태로 가져옵니다.\n","dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)"],"metadata":{"id":"hmdLWyHQoTKF","executionInfo":{"status":"ok","timestamp":1755411754612,"user_tz":-540,"elapsed":145,"user":{"displayName":"똥깡아제","userId":"13219049976116428546"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5cd1b542-eb35-4747-f168-6cace2ab6338"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["65 unique characters\n"]}]},{"cell_type":"markdown","source":["모든 글쓰기의 시작은 좋은 책을 읽는 것이죠. 우리 모델을 위한 '독서'와 '단어장 만들기' 과정입니다.\n","\n","- 데이터 다운로드 및 읽기: 셰익스피어 전집 텍스트 파일을 인터넷에서 다운로드합니다.\n","\n","- 단어장(Vocabulary) 만들기: 텍스트에 사용된 모든 글자(알파벳, !, ?, 띄어쓰기 등)를 중복 없이 모아 **vocab**이라는 단어장을 만듭니다. 그리고 각 글자에 고유 번호를 붙여 char2idx(글자->숫자), idx2char(숫자->글자)라는 두 개의 변환 사전을 만듭니다.\n","\n","- 텍스트를 숫자로 변환: 셰익스피어 텍스트 전체를 char2idx 사전을 이용해 숫자의 나열로 바꿉니다. 이제 컴퓨터가 글을 읽을 수 있게 되었죠.\n","\n","- 문제집 만들기 (split_input_target): 여기가 Char-RNN의 핵심입니다! 긴 숫자 나열을 seq_length(100) 길이로 자른 뒤, **'100글자짜리 문제'**와 '그 문제보다 한 글자 뒤에서 시작하는 100글자짜리 정답' 세트로 만듭니다. (예: [H,E,L,L]가 문제면 [E,L,L,O]가 정답)\n","\n","- 최종 데이터 파이프라인: 이 문제집들을 섞고, batch_size(64)만큼 묶어서 모델에 공급할 최종 준비를 마칩니다.\n","\n"],"metadata":{"id":"FvVKeMJ4oojM"}},{"cell_type":"markdown","source":["## 4. RNN 학습 모델 정의"],"metadata":{"id":"_NOXpfvWpBsE"}},{"cell_type":"code","source":["# tf.keras.Model을 상속받아 RNN 모델을 직접 정의합니다.\n","class RNN(tf.keras.Model):\n","\n","    # 1. 모델 생성자 (__init__) - 모델이 사용할 신경망 레이어(층)들을 미리 정의하고 초기화하는 부분입니다.\n","    def __init__(self, batch_size):\n","        super(RNN, self).__init__()\n","        # 단어를 숫자 벡터로 변환하는 임베딩 레이어를 정의합니다.\n","        self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        # 순차적인 데이터의 패턴을 학습하는 LSTM 레이어를 정의합니다.\n","        self.hidden_layer_1 = tf.keras.layers.LSTM(hidden_size,\n","                                                   return_sequences=True, # 모든 시점의 출력을 반환하도록 설정합니다.\n","                                                   stateful=True,         # 배치의 순서를 기억하도록 설정합니다.\n","                                                   recurrent_initializer='glorot_uniform')\n","        # 최종적으로 각 단어가 정답일 확률을 계산하는 출력 레이어를 정의합니다.\n","        self.output_layer = tf.keras.layers.Dense(vocab_size)\n","\n","    # 2. 모델 빌드 (build)\n","    # 실제 데이터가 입력되었을 때, 그 데이터의 크기에 맞춰 각 레이어의 가중치(weight)를 생성하는 부분입니다.\n","    def build(self, input_shape):\n","        # 첫 번째 레이어(Embedding)를 주어진 입력 shape으로 빌드합니다.\n","        self.embedding_layer.build(input_shape)\n","\n","        # Embedding 레이어의 출력 shape을 계산하여 다음 LSTM 레이어의 입력 shape으로 사용합니다.\n","        lstm_input_shape = self.embedding_layer.compute_output_shape(input_shape)\n","        self.hidden_layer_1.build(lstm_input_shape)\n","\n","        # LSTM 레이어의 출력 shape을 계산하여 다음 Dense 레이어의 입력 shape으로 사용합니다.\n","        dense_input_shape = self.hidden_layer_1.compute_output_shape(lstm_input_shape)\n","        self.output_layer.build(dense_input_shape)\n","\n","        # 모든 하위 레이어를 빌드한 후, 모델 자체를 빌드 완료 상태로 만듭니다.\n","        self.built = True\n","\n","    # 3. 순전파 로직 (call)\n","    # 데이터가 입력되었을 때, __init__에서 정의한 레이어들을 어떤 순서로 통과할지 결정하는 부분입니다.\n","    def call(self, x):\n","        # 입력 데이터를 임베딩 레이어에 통과시킵니다.\n","        embedded_input = self.embedding_layer(x)\n","        # 임베딩된 결과를 LSTM 레이어에 통과시킵니다.\n","        features = self.hidden_layer_1(embedded_input)\n","        # LSTM의 최종 결과를 출력 레이어에 통과시켜 예측값을 얻습니다.\n","        logits = self.output_layer(features)\n","\n","        return logits\n"],"metadata":{"id":"tMJzy4mhdRkq","executionInfo":{"status":"ok","timestamp":1755411756918,"user_tz":-540,"elapsed":26,"user":{"displayName":"똥깡아제","userId":"13219049976116428546"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["셰익스피어의 문체를 학습할 우리 모델의 '뇌'를 설계하는 부분입니다.\n","\n","- Embedding 레이어: 원-핫 인코딩의 단점을 해결하기 위해, 각 글자의 숫자 ID를 의미를 함축한 embedding_dim(256) 차원의 밀집 벡터로 변환합니다.\n","\n","- LSTM 레이어: 이 모델의 심장입니다. RNN의 단기 기억 문제를 해결한 LSTM을 사용하여 문장의 장기적인 맥락을 기억하고 학습합니다.\n","\n","- return_sequences=True: 각 타임스텝(글자)마다 출력을 내보내서, 다음 층으로 모든 시퀀스 정보를 전달합니다.\n","\n","- stateful=True: 현재 배치의 최종 기억 상태를 다음 배치의 초기 기억 상태로 이어주는 옵션으로, 긴 글을 학습할 때 유용합니다.\n","\n","- Dense 출력 레이어: LSTM이 전달한 맥락 정보를 바탕으로, **다음에 올 글자가 단어장(vocab_size)에 있는 각 글자일 확률(점수)**을 출력합니다."],"metadata":{"id":"dIvjN0Ppp4a0"}},{"cell_type":"markdown","source":["## 5. 손실 함수 및 최적화 함수 정의"],"metadata":{"id":"wnMLwKB5Q4-9"}},{"cell_type":"code","source":["# sparse cross-entropy 손실 함수를 정의합니다.\n","def sparse_cross_entropy_loss(labels, logits):\n","  return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True))\n","\n","# 최적화를 위한 Adam 옵티마이저를 정의합니다.\n","optimizer = tf.keras.optimizers.Adam()\n","\n","# 최적화를 위한 function을 정의합니다.\n","def train_step(model, input, target):\n","  with tf.GradientTape() as tape:\n","    logits = model(input)\n","    loss = sparse_cross_entropy_loss(target, logits)\n","  grads = tape.gradient(loss, model.trainable_variables)\n","  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","  return loss"],"metadata":{"id":"pCQkjzt7qGOb","executionInfo":{"status":"ok","timestamp":1755412249818,"user_tz":-540,"elapsed":4,"user":{"displayName":"똥깡아제","userId":"13219049976116428546"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["- 손실 함수 정의 :\n","모델의 예측이 실제 정답과 얼마나 다른지를 측정하는 손실 함수를 정의합니다. 여기서는 정수 형태의 정답 레이블을 바로 사용할 수 있는 sparse_categorical_crossentropy를 사용하여 모델의 오차를 계산합니다.\n","\n","- 옵티마이저 선택 :\n","계산된 손실(오차)을 효과적으로 줄여나가기 위한 최적화 도구로 Adam 옵티마이저를 선택합니다. Adam은 현재 가장 성능이 좋고 널리 사용되는 최적화 알고리즘 중 하나로, 모델의 파라미터를 효율적으로 업데이트해 줍니다.\n","\n","- 학습 단계 함수화 :\n","실제 학습의 한 단계를 train_step 이라는 함수로 묶어 정의하며, @tf.function을 붙여 텐서플로우가 더 빠르게 연산하도록 만듭니다. 이 함수는 데이터를 입력받아 오차를 계산하고, 그 오차를 바탕으로 옵티마이저가 모델을 올바른 방향으로 개선(업데이트)하는 모든 과정을 수행합니다."],"metadata":{"id":"FAJCaHlqCbp5"}},{"cell_type":"markdown","source":["## 6.텍스트 생성 함수 정의"],"metadata":{"id":"cNb-VgMxSXO7"}},{"cell_type":"code","source":["def generate_text(model, start_string):\n","  num_sampling = 4000  # 생성할 글자(Character)의 개수를 지정합니다.\n","\n","  # start_sting을 integer 형태로 변환합니다.\n","  input_eval = [char2idx[s] for s in start_string]\n","  input_eval = tf.expand_dims(input_eval, 0)\n","\n","  # 샘플링 결과로 생성된 string을 저장할 배열을 초기화합니다.\n","  text_generated = []\n","\n","  # 낮은 temperature 값은 더욱 정확한 텍스트를 생성합니다.\n","  # 높은 temperature 값은 더욱 다양한 텍스트를 생성합니다.\n","  temperature = 1.0\n","\n","  # 여기서 batch size = 1 입니다.\n","  model.hidden_layer_1.reset_states() # Reset the state of the LSTM layer\n","  for i in range(num_sampling):\n","    predictions = model(input_eval)\n","    # 불필요한 batch dimension을 삭제합니다.\n","    predictions = tf.squeeze(predictions, 0)\n","\n","    # 모델의 예측결과에 기반해서 랜덤 샘플링을 하기위해 categorical distribution을 사용합니다.\n","    predictions = predictions / temperature\n","    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","    # 예측된 character를 다음 input으로 사용합니다.\n","    input_eval = tf.expand_dims([predicted_id], 0)\n","    # 샘플링 결과를 text_generated 배열에 추가합니다.\n","    text_generated.append(idx2char[predicted_id])\n","\n","  return (start_string + ''.join(text_generated))"],"metadata":{"id":"9HP-O5TvpbSo","executionInfo":{"status":"ok","timestamp":1755412252101,"user_tz":-540,"elapsed":3,"user":{"displayName":"똥깡아제","userId":"13219049976116428546"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["학습이 끝난 모델로 실제로 글을 쓰는, 가장 흥미로운 부분입니다.\n","\n","- 시작 단어 제공: main 호출부에서 start_string으로 'ROMEO:' 같은 시작 글귀를 줍니다.\n","\n","- 예측 및 샘플링: 모델은 이 글귀를 보고 다음에 올 글자들의 확률을 예측합니다. 여기서 argmax로 무조건 가장 확률 높은 글자를 고르는 대신, **tf.random.categorical**을 사용해 **확률에 기반한 '뽑기'**를 합니다. 이렇게 해야 매번 조금씩 다른, 더 창의적인 글이 생성됩니다.\n","\n","- temperature: 이 '뽑기'의 창의성을 조절하는 변수입니다. 값이 낮으면 확률 높은 단어만 뽑아서 안정적인 글을 쓰고, 값이 높으면 확률 낮은 단어도 과감하게 뽑아서 더 독창적이지만 이상한 글을 쓸 수 있습니다.\n","\n","- 되먹임 루프: 방금 뽑은 글자를 다시 모델의 입력으로 넣어주고, 그 다음 글자를 예측하는 과정을 num_sampling(4000)번 반복합니다. 모델이 스스로 쓴 글을 읽고 다음 글을 계속 이어나가는 '자율 창작' 과정이죠."],"metadata":{"id":"fFxyrNLopb3_"}},{"cell_type":"markdown","source":["## 7."],"metadata":{"id":"U0XjiBcOq_Ue"}},{"cell_type":"code","source":["import glob\n","\n","def main(argv):\n","  del argv # Unyused.\n","  # Recurrent Neural Networks(RNN) 모델을 선언합니다.\n","  RNN_model = RNN(batch_size=batch_size)\n","\n","  # 데이터 구조 파악을 위해서 예제로 임의의 하나의 배치 데이터 에측하고, 예측결과를 출력합니다.\n","  for input_example_batch, target_example_batch in dataset.take(1):\n","    example_batch_predictions = RNN_model(input_example_batch)\n","    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n","\n","  # 모델 정보를 출력합니다.\n","  RNN_model.summary()\n","\n","  # checkpoint 데이터를 저장할 경로를 지정합니다.\n","  checkpoint_dir = './training_checkpoints'\n","  checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n","\n","  # Create checkpoint directory if it doesn't exist\n","  if not os.path.exists(checkpoint_dir):\n","      os.makedirs(checkpoint_dir)\n","\n","  for epoch in range(num_epochs):\n","    start = time.time()\n","\n","    # 매 반복마다 hidden state를 초기화합니다. (최초의 hidden 값은 None입니다.)\n","    # Access the stateful LSTM layer and call its reset_states method\n","    RNN_model.hidden_layer_1.reset_states()\n","\n","    for (batch_n, (input, target)) in enumerate(dataset):\n","      loss = train_step(RNN_model, input, target)\n","\n","      if batch_n % 100 == 0:\n","        template = 'Epoch {} Batch {} Loss {}'\n","        print(template.format(epoch+1, batch_n, loss))\n","\n","    # Save checkpoint every 5 epochs\n","    if (epoch + 1) % 5 == 0:\n","      RNN_model.save_weights(checkpoint_prefix.format(epoch=epoch))\n","\n","    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss)) # Removed .numpy()\n","    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n","\n","  RNN_model.save_weights(checkpoint_prefix.format(epoch=epoch))\n","  print(\"트레이닝이 끝났습니다!\")\n","\n","  # 텍스트 생성을 위해 배치 크기가 1인 모델을 새로 생성합니다.\n","  sampling_RNN_model = RNN(batch_size=1)\n","\n","  # 1. 체크포인트 디렉토리에서 모든 .weights.h5 파일을 찾습니다.\n","  checkpoint_files = glob.glob(os.path.join(checkpoint_dir, \"*.weights.h5\"))\n","\n","  # 2. 파일이 존재하는지 확인하고, 그중 가장 최신 파일(이름순으로 정렬했을 때 마지막 파일)을 찾습니다.\n","  if checkpoint_files:\n","    latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n","    print(f\"가장 최신 체크포인트를 로드합니다: {latest_checkpoint}\")\n","\n","    # 샘플링 모델을 빌드하고 가중치를 로드합니다.\n","    sampling_RNN_model.build(tf.TensorShape([1, None]))\n","    sampling_RNN_model.load_weights(latest_checkpoint)\n","  else:\n","    print(\"저장된 체크포인트를 찾을 수 없습니다.\")\n","\n","  print(\"\\n--- 샘플링 모델 구조 ---\")\n","  sampling_RNN_model.summary()\n","\n","  # 샘플링을 시작합니다.\n","  print(\"\\n샘플링을 시작합니다!\")\n","  print(generate_text(sampling_RNN_model, start_string=u'ROMEO: '))\n","\n","if __name__ == '__main__':\n","  # main 함수를 호출합니다.\n","  main(None)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"B9O4L38Bq_pu","executionInfo":{"status":"ok","timestamp":1755412467161,"user_tz":-540,"elapsed":213078,"user":{"displayName":"똥깡아제","userId":"13219049976116428546"}},"outputId":"483a9025-6c55-46e7-92fd-7783efa2a473"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"rnn_6\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"rnn_6\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │        \u001b[38;5;34m16,640\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m)        │     \u001b[38;5;34m5,246,976\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m65\u001b[0m)          │        \u001b[38;5;34m66,625\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,246,976</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,625</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,330,241\u001b[0m (20.33 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,330,241</span> (20.33 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,330,241\u001b[0m (20.33 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,330,241</span> (20.33 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1 Batch 0 Loss 4.175854206085205\n","Epoch 1 Batch 100 Loss 2.260575294494629\n","Epoch 1 Loss 2.0484\n","Time taken for 1 epoch 17.76358389854431 sec\n","\n","Epoch 2 Batch 0 Loss 2.1567423343658447\n","Epoch 2 Batch 100 Loss 1.7683178186416626\n","Epoch 2 Loss 1.7240\n","Time taken for 1 epoch 16.455842971801758 sec\n","\n","Epoch 3 Batch 0 Loss 1.7072826623916626\n","Epoch 3 Batch 100 Loss 1.5836668014526367\n","Epoch 3 Loss 1.5274\n","Time taken for 1 epoch 15.479090452194214 sec\n","\n","Epoch 4 Batch 0 Loss 1.542506456375122\n","Epoch 4 Batch 100 Loss 1.5246264934539795\n","Epoch 4 Loss 1.4527\n","Time taken for 1 epoch 15.344106435775757 sec\n","\n","Epoch 5 Batch 0 Loss 1.4772770404815674\n","Epoch 5 Batch 100 Loss 1.4711716175079346\n","Epoch 5 Loss 1.4488\n","Time taken for 1 epoch 16.45171594619751 sec\n","\n","Epoch 6 Batch 0 Loss 1.4092642068862915\n","Epoch 6 Batch 100 Loss 1.3711830377578735\n","Epoch 6 Loss 1.4113\n","Time taken for 1 epoch 15.672587394714355 sec\n","\n","Epoch 7 Batch 0 Loss 1.3632898330688477\n","Epoch 7 Batch 100 Loss 1.3505868911743164\n","Epoch 7 Loss 1.3427\n","Time taken for 1 epoch 15.705426931381226 sec\n","\n","Epoch 8 Batch 0 Loss 1.2963899374008179\n","Epoch 8 Batch 100 Loss 1.3177462816238403\n","Epoch 8 Loss 1.3349\n","Time taken for 1 epoch 16.285892009735107 sec\n","\n","Epoch 9 Batch 0 Loss 1.3155450820922852\n","Epoch 9 Batch 100 Loss 1.2816427946090698\n","Epoch 9 Loss 1.3370\n","Time taken for 1 epoch 15.473163843154907 sec\n","\n","Epoch 10 Batch 0 Loss 1.246812343597412\n","Epoch 10 Batch 100 Loss 1.2418646812438965\n","Epoch 10 Loss 1.2701\n","Time taken for 1 epoch 20.51194405555725 sec\n","\n","트레이닝이 끝났습니다!\n","가장 최신 체크포인트를 로드합니다: ./training_checkpoints/ckpt_9.weights.h5\n","\n","--- 샘플링 모델 구조 ---\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"rnn_7\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"rnn_7\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding_7 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │        \u001b[38;5;34m16,640\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)        │     \u001b[38;5;34m5,246,976\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)          │        \u001b[38;5;34m66,625\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,246,976</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,625</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,330,241\u001b[0m (20.33 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,330,241</span> (20.33 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,330,241\u001b[0m (20.33 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,330,241</span> (20.33 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","샘플링을 시작합니다!\n","ROMEO: Parians.\n","\n","CORIOLANUS:\n","To yourself,\n","I would be king.\n","\n","ANGELO:\n","Thy love, whilve I bend myself,\n","And thou reterp'st thou offended:\n","Think it they was; 'Signiar Lucentio;\n","You do die to both his corse: thou liest;\n","Glas, fortune's mother than you had many day, for the worve\n","Shows here's noble grace who till he should resolute\n","e't women shall were almost his house,\n","But but yonder the Werish forfiverers: bring you, like thee to yours,\n","Nor come, treacherous lie.\n","\n","LEONTES:\n","No try bile?\n","\n","MONTAGUE:\n","An it as token's a salt good meltary.\n","\n","GONZALO:\n","And more is my safe?\n","\n","BoyO:\n","Al what dissesses' else?\n","O pitious embario's\n","Cheeronood sudden Roma, the order his,\n","As they were like to love to Romeo.\n","\n","GLOUCESTER:\n","The loss this prescain can combled this son terror\n","We'll have her more to disepute of my close't men?\n","O, king! by your crest, and if he is succeed\n","Say Hermione will write grave.\n","\n","BISABETH:\n","Ay, soft against the Dreas cheek;\n","And had he death and honour,' tell Chindiser\n","balls, though raised ine is yours, grief with what them.\n","\n","DUCHESS OF YORK:\n","Down will I, this is the end of my women\n","Unbicion, and to reason me!\n","And every breathest hed more stace for year's pale.\n","\n","POMPEY:\n","Pry them, ere I stout her from our worses,\n","Thou close to live.\n","\n","WARWICK:\n","Than Edward, soul Isabel well he hath\n","An honesty advantage of the sont,\n","And nf the terms go with your trwaces foreardome;\n","Adal news, as you'll grink Bemona,\n","A year, good my precest, not for your or a gain;\n","And then it was at hand, do not seen you.\n","\n","CLIFFORD:\n","Even Richard, hark'd in me! I will tit upon her,\n","Which was the one the time; them all home is my son\n","Are men bit disposs: his father's verge\n","Mis--I will, as I promise of all me;\n","By the over-pereful arguancements,\n","Stay with a daughter and the world should\n","King Lucengeth higher velvet; and to him,\n","And come to visit our knave of the contrals shall not\n","be fasted good, because home we foe,\n","Since who axternoof and brother's tail,\n","I pray you, sir,\n","Whom you comes the bent; as the power lose, the son\n","prockers are Prince, farewere, all of him to die;\n","Even to the either part for all his son.\n","\n","BIANCA:\n","That go with grave alast your threefets we may ballad him\n","The fear of power's contempt will not sooner the royal lose, I say, let us of us!\n","In here, so thanks, he looks your honour.\n","\n","Second Let Clarence hath a leave, head again\n","By all the Volsces to dist; as they still his bosom:\n","Thus I pill were Taunt in the prince,\n","Where my deservise and profit saves\n","Were new incense; all so too ruse; but pillain the most you\n","a praisenant with our hearting of these eir,\n","Drown me were nexted at pityour too,\n","For somender ship! Edward he yet have\n","Unteeds for never less died to the prison!\n","\n","DUKE VINCCENTIO:\n","So for you, my lord? I had your child unscaugaty, for thee\n","I will hear more age aughty grown stock 'More in the\n","name that hath made no the hatch vile victory,\n","Of bright a villain, who was my person\n","Endured of purpose, stain on me, as he doth return\n","Then take you than I charge your hands,\n","And rather off the sounder, Hereforth you,\n","Your acticious love is bear.\n","\n","ANTONIO:\n","Why, 'tis yound ours.\n","\n","Second Hup you, wither borness to thee\n","To meet thee, or with such it dead?\n","\n","SEBOSTIAN:\n","Hold, go with thee, King Henry tear them laugh to the prince.\n","\n","KATHARINA:\n","Yea,-forture, for a very, sir,\n","That I want myself, that lack him! what nobies\n","How is much plants before him so much else and weep.\n","\n","HERMIONE:\n","'Tis they so, so good Camillo,\n","Merely have too boce: 'twas Somerember\n","Your knups i' the souls of life, I heard thou wERDY:\n","While news deliver?\n","\n","BAPSTIAN:\n","Who knows your followers was in full\n","Than your father throne, 'twas 'scapore's; if he should beat\n","The honouratestirs of the very fled,\n","That loyalty oak Richard's noble oaths?\n","\n","QUEEN ELIZABETH:\n","Good morrow with the streas off.\n","Gail he says to door?\n","\n","PETRUCHIO:\n","Nay, he willing find the whole fought be leish.\n","\n","ROMEO:\n","Thou durbour that serves very head, have you\n","hate, a furn extremity;\n","This babb'd stamp'd souls' talk forth letters;\n","Even is our own fear\n"]}]},{"cell_type":"markdown","source":["#[참고] Gen AI로 재구성"],"metadata":{"id":"aAZxc1IBwmUl"}},{"cell_type":"markdown","source":["## '완벽한' 대본으로 재구성 (Reconstruction)\n","원본 내용은 의미가 없기 때문에, 원본의 등장인물과 분위기를 살려 의미가 통하는 짧은 한 장면으로 재창작해 보았습니다. 로미오의 낭만주의와 코리올라누스의 현실주의가 충돌하는 장면입니다.\n","\n","(Original English Text, Reconstructed for Clarity and Meaning)\n","\n","ROMEO:\n","Speak not of Paris, friend, nor of its fleeting charms.\n","A city built on whispered love and fragile art,\n","It cannot hold a candle to the heart of Rome.\n","\n","CORIOLANUS:\n","Paris? A trifle! A kingdom made for poets and for fools!\n","You speak of love, while I see only shifting thrones.\n","In Rome, a man might build himself a crown of stone;\n","To be a king of men—that is a worthy cause.\n","\n","ROMEO:\n","And yet, a king of what? Of dust and hollow pride?\n","I would sooner be the king of one true, loyal heart\n","Than rule an empire built on fear and bitter wars.\n","\n","CORIOLANUS:\n","A fool's choice, Romeo. A heart is fickle and may break.\n","An empire, forged in steel and command, is eternal."],"metadata":{"id":"_8cU9hHO09Yp"}},{"cell_type":"markdown","source":["## 재구성된 대본 번역 (Translation)\n","위에서 재구성한 대본을 현대적인 한국어로 자연스럽게 번역한 내용입니다.\n","\n","(재구성된 대본 한국어 번역)\n","\n","로미오:\n","친구여, 파리에 대해 말하지 말게, 그 덧없는 매력에 대해서도.\n","속삭이는 사랑과 부서지기 쉬운 예술로 세워진 그 도시는\n","로마의 심장에 비하면 촛불 하나조차 되지 못한다네.\n","\n","코리올라누스:\n","파리라고? 하찮은 것! 시인과 어리석은 자들을 위한 왕국일 뿐!\n","그대는 사랑을 말하지만, 내 눈에는 흔들리는 왕좌만 보이는군.\n","로마에서는 사나이 스스로 돌로 된 왕관을 만들 수 있지.\n","사람들의 왕이 되는 것, 그것이야말로 가치 있는 대의다.\n","\n","로미오:\n","허나 무엇의 왕이 되겠는가? 먼지와 공허한 자존심의 왕?\n","나는 두려움과 쓰라린 전쟁으로 세운 제국을 다스리느니,\n","차라리 진실하고 충성스러운 마음 하나의 왕이 되겠네.\n","\n","코리올라누스:\n","어리석은 선택이군, 로미오. 마음은 변덕스러워 부서지기 마련이지만,\n","강철과 명령으로 벼려낸 제국은 영원한 법일세."],"metadata":{"id":"TDNFPsDG1CEo"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"fdobF_OatD5c"}}]}